import pandas as pd
import logging
import collections

from gensim.corpora.textcorpus import TextCorpus

logger = logging.getLogger(__name__)

class GenericTextCorpus(TextCorpus):

    def __init__(self, stream, dictionary=None, metadata=False, character_filters=None, tokenizer=None, token_filters=None, bigram_transform=False):
        self.stream = stream
        self.filenames = None
        self.documents = None
        self.length = None

        #if 'filenames' in content_iterator.__dict__:
        #    self.filenames = content_iterator.filenames
        #    self.document_names = self._compile_documents()
        #    self.length = len(self.filenames)

        token_filters = self.default_token_filters() + (token_filters or [])

        #if bigram_transform is True:
        #    train_corpus = GenericTextCorpus(content_iterator, token_filters=[ x.lower() for x in tokens ])
        #    phrases = gensim.models.phrases.Phrases(train_corpus)
        #    bigram = gensim.models.phrases.Phraser(phrases)
        #    token_filters.append(
        #        lambda tokens: bigram[tokens]
        #    )

        super(GenericTextCorpus, self).__init__(
            input=True,
            dictionary=dictionary,
            metadata=metadata,
            character_filters=character_filters,
            tokenizer=tokenizer,
            token_filters=token_filters
        )

    def default_token_filters(self):
        return [
            (lambda tokens: [ x.lower() for x in tokens ]),
            (lambda tokens: [ x for x in tokens if any(map(lambda x: x.isalpha(), x)) ])
        ]

    def getstream(self):
        """Generate documents from the underlying plain text collection (of one or more files).
        Yields
        ------
        str
            Document read from plain-text file.
        Notes
        -----
        After generator end - initialize self.length attribute.
        """

        document_infos = []
        for filename, content in self.stream:
            yield content
            document_infos.append({
                'document_name': filename
            })

        self.length = len(document_infos)
        self.documents = pd.DataFrame(document_infos)
        self.filenames = list(self.documents.document_name.values)

    def get_texts(self):
        '''
        This is mandatory method from gensim.corpora.TextCorpus. Returns stream of documents.
        '''
        for document in self.getstream():
            yield self.preprocess_text(document)

    def preprocess_text(self, text):
        """Apply `self.character_filters`, `self.tokenizer`, `self.token_filters` to a single text document.

        Parameters
        ---------
        text : str
            Document read from plain-text file.

        Returns
        ------
        list of str
            List of tokens extracted from `text`.

        """
        for character_filter in self.character_filters:
            text = character_filter(text)

        tokens = self.tokenizer(text)
        for token_filter in self.token_filters:
            tokens = token_filter(tokens)

        return tokens

    def __get_document_info(self, filename):
        return {
            'document_name': filename,
        }

    def ___compile_documents(self):

        document_data = map(self.__get_document_info, self.filenames)

        documents = pd.DataFrame(list(document_data))
        documents.index.names = ['document_id']

        return documents
