{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2132a567",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/cluster.vq.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8de71",
   "metadata": {},
   "source": [
    "## Word Distribution Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4569f5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=wrong-import-position, import-error\n",
    "\n",
    "import __paths__  # isort: skip\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import holoviews as hv\n",
    "import ipywidgets\n",
    "import penelope.common.goodness_of_fit as gof\n",
    "import penelope.corpus.dtm as vectorized_corpus\n",
    "import penelope.notebook.cluster_analysis.cluster_analysis_gui as cluster_analysis_gui\n",
    "import penelope.notebook.word_trends as word_trends\n",
    "from bokeh.plotting import output_notebook\n",
    "from IPython.display import display\n",
    "\n",
    "root_folder = __paths__.ROOT_FOLDER\n",
    "corpus_folder = os.path.join(root_folder, \"output\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e41f5c",
   "metadata": {},
   "source": [
    "## Load previously vectorized corpus\n",
    "\n",
    "The corpus was created using the following settings:\n",
    " - Tokens were converted to lower case.\n",
    " - Only tokens that contains at least one alphanumeric character (only_alphanumeric).\n",
    " - Accents are ot removed (remove_accents)\n",
    " - Min token length 2 (min_len)\n",
    " - Max length not set (max_len)\n",
    " - Numerals are removed (keep_numerals, -N)\n",
    " - Symbols are removed (keep_symbols, -S)\n",
    "\n",
    "Use the `corpus_vectorizer` module to create a new corpus with different settings.\n",
    "\n",
    "The loaded corpus is processed in the following ways:\n",
    "\n",
    " - Exclude tokens having a total word count less than 10000\n",
    " - Include at most 50000 most frequent words words.\n",
    " - Group documents by year (y_corpus).\n",
    " - Normalize token count based on each year's total token count.\n",
    " - Normalize token distrubution over years to 1.0 (n_corpus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c26e4b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "y_corpus = vectorized_corpus.load_corpus(\n",
    "    tag=\"SOU_test_L0_+N_+S\",\n",
    "    folder=os.path.join(root_folder, \"output\"),\n",
    "    n_count=5000,\n",
    "    n_top=50000,\n",
    "    axis=1,\n",
    "    keep_magnitude=False,\n",
    ")\n",
    "\n",
    "n_corpus = y_corpus.normalize(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee8cfb",
   "metadata": {},
   "source": [
    "## Deviation metrics\n",
    "\n",
    "These metrics are used to identify tokens that have a distribution that deviates the most to a uniform distribution.\n",
    "The following metrics are computed for each token:\n",
    "\n",
    "| Metric |  | |\n",
    "| :------ | :------- | :------ |\n",
    "| L2-norm | Measures distance to unform distribution. Lower bound 1/sqrt(d) is uniformity and upper bound is a 1-hot vector. |\n",
    "| Linear regression | Curve fitting to f(x) = k * x + m, where k is the slope, and m is the intercept to y axis when x is 0. A uniform curve has slope equals to 0. |\n",
    "| χ2 test | [Khan](https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/pearson-s-chi-square-test-goodness-of-fit) [Blog](https://towardsdatascience.com/inferential-statistic-understanding-hypothesis-testing-using-chi-square-test-eacf9fcac533) [Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test) |\n",
    "| Statistics | The min, max, mean and variance of the distribution |\n",
    "| Earth mover distance | [PDF](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf) [Wikipedia](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)  |\n",
    "| Kullback-Leibler divergence | S = sum(pk * log(pk / qk), where pk is the token distribution and qk is the uniform distribution |\n",
    "| Entropy | Basically the same as KLD |\n",
    "| Skew | A measure of the \"skewness\" of the token distribution. See [scipy.stats.skew](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html#scipy.stats.skew) |\n",
    "\n",
    "References:\n",
    "\n",
    " - [StackExchange](stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution)\n",
    "    \"It just so happens, though, that the L2 norm has a simple algebraic connection to the χ2 statistic used in goodness of fit tests:\n",
    "    that's the reason it might be suitable to measure non-uniformity\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0341b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# pylint: disable=redefined-outer-name\n",
    "\n",
    "\n",
    "def display_uniformity_metrics(x_corpus, df_gof, df_most_deviating):\n",
    "\n",
    "    output_row = [ipywidgets.Output(), ipywidgets.Output()]\n",
    "    display(ipywidgets.HBox(output_row))\n",
    "\n",
    "    with output_row[0]:\n",
    "\n",
    "        columns = [\n",
    "            \"token\",\n",
    "            \"word_count\",\n",
    "            \"l2_norm\",\n",
    "            \"slope\",\n",
    "            \"chi2_stats\",\n",
    "            \"earth_mover\",\n",
    "            \"kld\",\n",
    "            \"skew\",\n",
    "        ]\n",
    "        display(df_gof.nlargest(10000, columns=[\"word_count\"])[columns])\n",
    "\n",
    "    with output_row[1]:\n",
    "        pass\n",
    "        # columns = ['L2 token', 'L2', 'K token', 'K', 'X2 token', 'X2', 'EMD token', 'EMD', 'KLD token', 'KLD', 'Ent. token', 'Entropy']\n",
    "        # df_most_deviating.columns = columns\n",
    "        # display(df_most_deviating.head(100))\n",
    "\n",
    "    gof.plot_metrics(df_gof)\n",
    "    gof.plot_slopes(x_corpus, df_most_deviating, \"l2_norm\")\n",
    "\n",
    "    # df_gof.to_csv('df_gof.txt', sep='\\t')\n",
    "    # df_most_deviating.to_csv('df_most_deviating.txt', sep='\\t')\n",
    "\n",
    "\n",
    "# n_corpus.data.shape[0]\n",
    "\n",
    "df_gof = gof.compute_goddness_of_fits_to_uniform(n_corpus)\n",
    "df_most_deviating = gof.compile_most_deviating_words(df_gof, n_count=10000)\n",
    "\n",
    "\n",
    "display_uniformity_metrics(n_corpus, df_gof, df_most_deviating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da89350",
   "metadata": {},
   "source": [
    "## Word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d7328",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = df_most_deviating[\"l2_norm_token\"]\n",
    "word_trends.TrendsWithPickTokensGUI.create(n_corpus, tokens).layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab13d66",
   "metadata": {},
   "source": [
    "### Cluster Analysis (Hierarchical Clustering & K-means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac882a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "container = cluster_analysis_gui.display_gui(n_corpus, df_gof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327036a7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "### Some references\n",
    "\n",
    "|  |  |\n",
    "| :------- | :------- |\n",
    "| Overview: | https://docs.scipy.org/doc/scipy-0.19.0/reference/cluster.hierarchy.html |\n",
    "| Linkage: |https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage |\n",
    "| Reference | Daniel Mullner, “Modern hierarchical, agglomerative clustering algorithms”, [arXiv:1109.2378v1](https://arxiv.org/abs/1109.2378v1). |\n",
    "|  | [Link](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/) [Link](https://github.com/giusdp/Clustering-Techniques/tree/0c78d1a893995c4603ed07216d645801ab4fdb4d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea53239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
