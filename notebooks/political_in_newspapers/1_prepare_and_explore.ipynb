{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34807672",
   "metadata": {},
   "source": [
    "### Process R data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500b0d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=redefined-outer-name,no-member\n",
    "\n",
    "import __paths__  # isort:skip pylint: disable=import-error, unused-import\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from notebooks.political_in_newspapers import corpus_data\n",
    "\n",
    "corpus_folder = '/data/westac/textblock_politisk'\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_meta_text_blocks_as_data_frame(folder):\n",
    "    \"\"\" Load censored corpus data \"\"\"\n",
    "\n",
    "    filename = os.path.join(folder, corpus_data.meta_textblocks_filename)\n",
    "    df_meta = pd.read_csv(filename, compression=\"zip\", header=0, sep=\",\", quotechar='\"', na_filter=False)\n",
    "    # df_meta = df_meta[['id', 'pred_bodytext']].drop_duplicates()\n",
    "    # df_meta.columns = [\"doc_id\", \"pred_bodytext\"]\n",
    "    # df_meta = df_meta.set_index(\"doc_id\")\n",
    "    return df_meta\n",
    "\n",
    "\n",
    "def load_reconstructed_text_corpus(folder):\n",
    "    filename: str = os.path.join(folder, corpus_data.reconstructed_text_corpus_file)\n",
    "    if not os.path.isfile(filename):\n",
    "        df_corpus: pd.DataFrame = corpus_data.load_corpus_dtm_as_data_frame(folder)\n",
    "        df_vocabulary: pd.DataFrame = corpus_data.load_vocabulary_file_as_data_frame(folder)\n",
    "        id2token = df_vocabulary[\"token\"].to_dict()\n",
    "        df_reconstructed_text_corpus: pd.DataFrame = (df_corpus.groupby(\"document_id\")).apply(\n",
    "            lambda x: \" \".join(flatten(x[\"tf\"] * (x[\"token_id\"].apply(lambda y: [id2token[y]]))))\n",
    "        )\n",
    "        df_reconstructed_text_corpus.to_csv(filename, compression=\"zip\", header=0, sep=\",\", quotechar='\"')  # type: ignore\n",
    "    else:\n",
    "        df_reconstructed_text_corpus: pd.DataFrame = pd.read_csv(filename, compression=\"zip\", header=None, sep=\",\", quotechar='\"')  # type: ignore\n",
    "        df_reconstructed_text_corpus.columns = [\"document_id\", \"text\"]\n",
    "        df_reconstructed_text_corpus.set_index(\"document_id\")\n",
    "\n",
    "    return df_reconstructed_text_corpus\n",
    "\n",
    "\n",
    "def plot_document_size_distribution(df_document):\n",
    "\n",
    "    df_term_counts = df_document.groupby(\"term_count\").size()\n",
    "\n",
    "    dx = pd.DataFrame({\"term_count\": list(range(0, df_term_counts.index.max() + 1))}).set_index(\"term_count\")\n",
    "    df_term_counts = dx.join(df_term_counts.rename(\"x\"), how=\"left\").fillna(0).astype(np.int)\n",
    "\n",
    "    ax = df_term_counts.plot.bar(figsize=(20, 10), rot=45)\n",
    "\n",
    "    ticks = ax.xaxis.get_ticklocs()\n",
    "    ticklabels = [lst.get_text() for lst in ax.xaxis.get_ticklabels()]\n",
    "    ax.xaxis.set_ticks(ticks[::100])\n",
    "    ax.xaxis.set_ticklabels(ticklabels[::100])\n",
    "\n",
    "    return df_term_counts\n",
    "\n",
    "\n",
    "def unique_documents_per_year_and_publication(df_document):\n",
    "    df = (\n",
    "        df_document.groupby([\"year\", \"publication\"])\n",
    "        .agg(document_count=(\"doc_id\", \"nunique\"))\n",
    "        .reset_index()\n",
    "        .set_index([\"year\", \"publication\"])\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def mean_tokens_per_year(df_document):\n",
    "    df = (\n",
    "        df_document.groupby([\"year\", \"publication\"])\n",
    "        .agg(term_count=(\"term_count\", \"mean\"))\n",
    "        .reset_index()\n",
    "        .set_index([\"year\", \"publication\"])\n",
    "        .unstack(\"publication\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf640c",
   "metadata": {},
   "source": [
    "### Load DTM, document index and vocabulary\n",
    "This data is loaded from CSV files exported from R (drm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056c133",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# df = load_meta_text_blocks_as_data_frame(corpus_folder)\n",
    "# rt = load_reconstructed_text_corpus(corpus_folder)\n",
    "\n",
    "df_corpus, df_document, df_vocabulary = corpus_data.load(corpus_folder)\n",
    "id2token = df_vocabulary[\"token\"].to_dict()\n",
    "\n",
    "df_tf = df_corpus.groupby([\"document_id\"]).agg({'term_count': (\"tf\", \"sum\")})\n",
    "df_document = df_document.merge(df_tf, how=\"inner\", right_index=True, left_on='document_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load DN 68, write Excel and ZP\n",
    "dn68 = df_document[(df_document.publication == 'DAGENS NYHETER') & (df_document.year == 1968)]\n",
    "rt = load_reconstructed_text_corpus(corpus_folder)\n",
    "\n",
    "dn68_text = rt.merge(dn68, how='inner', left_index=True, right_on='document_id')[\n",
    "    ['document_id', 'year', 'date', 'term_count', 'text']\n",
    "]\n",
    "dn68_text.columns = ['document_id', 'year', 'date', 'term_count', 'text']\n",
    "dn68_text.to_excel('dn68_text.xlsx')\n",
    "# dn68_text.to_csv('dn68_text.csv', sep='\\t')\n",
    "\n",
    "\n",
    "with zipfile.ZipFile('dn68.zip', 'w', zipfile.ZIP_DEFLATED) as out:\n",
    "    i = 0\n",
    "    for index, row in dn68_text.iterrows():\n",
    "        i += 1\n",
    "        filename = 'dn_{}_{}_{}.txt'.format(row['date'], index, 1)\n",
    "        text = str(row['text'])\n",
    "        out.writestr(filename, text, zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba4716",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Document size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789c240",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "_ = plot_document_size_distribution(df_document)\n",
    "\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d8ce1",
   "metadata": {},
   "source": [
    "### Number of documents per year and publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c4b8c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unique_yearly_docs = unique_documents_per_year_and_publication(df_document)\n",
    "\n",
    "unique_yearly_docs.unstack(\"publication\").plot(kind=\"bar\", subplots=True, figsize=(20, 20), layout=(2, 2), rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f659b",
   "metadata": {},
   "source": [
    "### Numer of tokens per year and publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425dfba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_mean_tokens_per_year = mean_tokens_per_year(df_document)\n",
    "df_mean_tokens_per_year.to_excel(\"mean_tokens_per_year.xlsx\")\n",
    "# display(df_mean_tokens_per_year)\n",
    "# df_mean_tokens_per_year.plot(kind='bar', subplots=True, figsize=(25,25), layout=(2,2), rot=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2a5db",
   "metadata": {},
   "source": [
    "### Print data sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da95035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Corpus metrics, source \"dtm1.rds\", arrays drm$i, drm$j, drm$v')\n",
    "print(\"  {} max document ID\".format(df_corpus.document_id.max()))\n",
    "print(\"  {} unique document ID\".format(df_corpus.document_id.unique().shape[0]))\n",
    "print(\"  {} max token ID\".format(df_corpus.token_id.max()))\n",
    "print(\"  {} unique token ID\".format(df_corpus.token_id.unique().shape[0]))\n",
    "\n",
    "print('Document metrics, source \"dtm1.rds\", arrays drm$dimnames[1]')\n",
    "print(\"  {} max ID\".format(df_document.index.max()))\n",
    "print(\"  {} unique ID\".format(df_document.index.unique().shape[0]))\n",
    "print(\"  {} unique names\".format(df_document.doc_id.unique().shape[0]))\n",
    "\n",
    "print('Vocabulary metrics, source \"dtm1.rds\", arrays drm$dimnames[2]')\n",
    "print(\"  {} max ID\".format(df_vocabulary.index.max()))\n",
    "print(\"  {} unique ID\".format(df_vocabulary.index.unique().shape[0]))\n",
    "print(\"  {} unique token\".format(df_vocabulary.token.unique().shape[0]))\n",
    "\n",
    "# df_document.groupby('doc_id').filter(lambda x: len(x) > 1).head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('welfare_state_analytics': pipenv)",
   "language": "python",
   "name": "python37564bitwelfarestateanalyticspipenvb857bd21a5fc4575b483276067dc0241"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
