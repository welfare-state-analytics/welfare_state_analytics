{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "state_filename: str = \"../../tests/test_data/train_0ec98113/mallet/state.mallet.gz\"\n",
    "state: pd.DataFrame = pd.read_csv(state_filename, compression='gzip', sep=' ', skiprows=[1,2] )\n",
    "\n",
    "#state.columns = ['doc','source','pos','typeindex','type','topic']\n",
    "#state.columns = ['document_id','source','pos','token_id','token','topic_id']\n",
    "state.columns = ['document_id','source','pos','w','token','z']\n",
    "\n",
    "vocab: dict = state[['w', 'token']].drop_duplicates().set_index('token',drop=True).w.to_dict()\n",
    "\n",
    "state = state[[\"document_id\", \"w\", \"z\", \"token\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend:\n",
    "\n",
    "| Symbol | | Description | | Dimension | |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| d in D | | A set of Documents, d is document-term counts | | | known |\n",
    "| W | | Vocabulary (unique tokens) | | | known |\n",
    "| K | | Number of topics | | | unknown |\n",
    "| z | | Topic assignment of every word in every document. From z we can derive the document topic mixture, ($\\theta$), and the word distribution, ($\\phi$), of each topic. | | | unknown |\n",
    "| theta | $\\theta$ | each row represent a distribution of topics over documents | $P(topic_k \\| document_d)$ | $K \\times D$ |\n",
    "| phi | $\\phi$ | Distribution of words over topics | $P(token_v\\|topic_k)$ | $ M \\times K $\n",
    "| gamma | $\\gamma$ | |  $P(topic_k\\|token_v)$|\n",
    "| alpha | $\\alpha$ | Input parameter of the dirichlet distribution used when infering $\\theta$ i.e. when sampling the *topic distribution of a document*. Prior information for document's topic mixture.|\n",
    "| beta | $\\beta$ |  Input parameter of the dirichlet distribution used when sampling the *word distribution of a given topic*. |\n",
    "\n",
    "The most important are three matrices: theta gives P(topick|documentd), phi gives P(tokenv|topick), and gamma gives P(topick|tokenv).\n",
    "\n",
    "\n",
    "Let's get the ugly part out of the way, the parameters and variables that are going to be used in the model. \n",
    "\n",
    "* <b>beta</b> ($\\overrightarrow{\\beta}$) : In order to determine the value of $\\phi$, the word distirbution of a given topic, we sample from a dirichlet distribution using $\\overrightarrow{\\beta}$ as the input parameter.  What does this mean? The $\\overrightarrow{\\beta}$ values are our prior information about the word distribution in a topic. Example: I am creating a document generator to mimic other documents that have topics labeled for each word in the doc. I can use the number of times each word was used for a given topic as the $\\overrightarrow{\\beta}$ values. \n",
    "\n",
    "* <b>theta</b> ($\\theta$) : Is the topic proportion of a given document. More importantly it will be used as the parameter for the multinomial distribution used to identify the topic of the next word. To clarify, the selected topic's word distribution will then be used to select a word _w_. \n",
    "\n",
    "* <b>phi</b> ($\\phi$) : Is the word distribution of each topic, i.e. the probability of each word in the vocabulary being generated if a given topic, _z_ (z ranges from 1 to k),  is selected.  \n",
    "\n",
    "* <b>xi</b> ($\\xi$) : In the case of a variable lenght document, the document length is determined by sampling from a Poisson distribution with an average length of $\\xi$\n",
    "\n",
    "* <b>k</b> : Topic index\n",
    "* <b>z</b> : Topic selected for the next word to be generated. \n",
    "* <b>w</b> : Generated Word\n",
    "* <b>d</b> : Current Document\n",
    "\n",
    "Outside of the variables above all the distributions should be familiar from the previous chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 248 3 sabatini\n"
     ]
    }
   ],
   "source": [
    "\n",
    "M = state[\"document_id\"].max()\n",
    "V = state[\"token_id\"].max() + 1\n",
    "K = state[\"topic_id\"].max() + 1\n",
    "\n",
    "Nd = np.zeros((M, K), dtype=int)\n",
    "Nk = np.zeros((K, V), dtype=int)\n",
    "\n",
    "pos = None\n",
    "\n",
    "Z = np.zeros(M, dtype=int) if pos else []\n",
    "\n",
    "for m in range(M):\n",
    "    \n",
    "    doc = state.loc[state[\"document_id\"] == m]\n",
    "    \n",
    "    if pos:\n",
    "        Z[m] = doc.iloc[pos[m]][\"z\"]\n",
    "        \n",
    "    for i, (doc, w, z, word) in doc.iterrows():\n",
    "        \n",
    "        Nd[doc,z] += 1\n",
    "        Nk[z,w] += 1\n",
    "        \n",
    "        if pos == None:\n",
    "            Z.append(z)\n",
    "\n",
    "print(doc, w, z, word)\n",
    "theta = Nd / Nd.sum(axis=1)[:,np.newaxis]\n",
    "phi = Nk / Nk.sum(axis=1)[:,np.newaxis]\n",
    "results = [theta, phi, Nd, Nk, Z]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.01010101, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.11188811, 0.        , 0.        , ..., 0.        , 0.00699301,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00763359],\n",
       "       [0.        , 0.01980198, 0.        , ..., 0.00990099, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print(f'Results retrieved after {round(time.time()-start)} seconds.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPS-LMU\t\t    jupyterhub\t   sandboxes\t    welfare-state-analytics\n",
      "clones\t\t    minio_sandbox  sead\t\t    workspaces\n",
      "data\t\t    penelope\t   tCoIR\n",
      "hypermodern-python  petprojects    vulcan.dotfiles\n",
      "inidun\t\t    polycaste\t   webapi\n"
     ]
    }
   ],
   "source": [
    "def get_results(state, pos=None):\n",
    "    \"\"\"\n",
    "    Retrieve model parameters and topic indicator counts from state dataframe.\n",
    "    Position returns topic indicators only for words in pos positions (count matrices are the same).\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    vocab = {}\n",
    "    V = len(set(state[\"w\"]))\n",
    "    K = len(set(state[\"z\"]))\n",
    "    Nd = np.zeros((M, K), dtype=int)\n",
    "    Nk = np.zeros((K, V), dtype=int)\n",
    "    Z = np.zeros(M, dtype=int) if pos else []\n",
    "    for m in range(M):\n",
    "        M = max(state[\"doc\"])+1\n",
    "        doc = state.loc[state[\"doc\"] == m]\n",
    "        if pos:\n",
    "            Z[m] = doc.iloc[pos[m]][\"z\"]\n",
    "        for i,row in doc.iterrows():\n",
    "            doc, w, z, word = row[[\"doc\", \"w\", \"z\", \"word\"]]\n",
    "            Nd[doc,z] += 1\n",
    "            Nk[z,w] += 1\n",
    "            if pos == None:\n",
    "                Z.append(z)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = w\n",
    "\n",
    "    theta = Nd / Nd.sum(axis=1)[:,np.newaxis]\n",
    "    phi = Nk / Nk.sum(axis=1)[:,np.newaxis]\n",
    "    results = [theta, phi, Nd, Nk, Z]\n",
    "    print(f'Results retrieved after {round(time.time()-start)} seconds.')\n",
    "    return [results, vocab]\n",
    "\n",
    "get_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('humlab-westac-O7wB9ikj-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 (main, Feb 17 2022, 14:08:33) \n[GCC 9.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3ca4cc9789cdd82ec87d95ee239b4d4ca4e6228091c5faf88f83d35a5b82483"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
