{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be6ee5d",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook implements a processing pipeline from computes word co-occurrences from plain text. The term-term co-occurrence matrix is transformed into a DTM corpus that has a vocabulary consisting of token-pairs. The co-occurring word trends can hence be xxplored using the ordinary word trends analysis tools.\n",
    "\n",
    "For large corpora the processing time can be considerable and in such a case you\n",
    "should consider using the CLI-version of the processing pipeline.\n",
    "\n",
    "### Text Processing Pipeline\n",
    "\n",
    "| | Building block | Arguments | Description | Configuration\n",
    "| -- | :------------- | :------------- | :------------- | :------------- |\n",
    "| âš™ | <b>SetTagger</b>~~SpacyModel~~ | tagger | Set PoS tagger | spaCy\n",
    "| ðŸ“œ| <b>LoadText</b> | reader_opts, transform_opts | Load text stream | config.yaml\n",
    "| ðŸ”Ž | <b>Tqdm</b> | âšª | Progress indicator | âšª\n",
    "| âŒ› | <b>Passthrough</b> | âšª | Passthrough  | âšª\n",
    "| ðŸ”¨ | Spacy<b>ToTaggedFrame</b> | tagger service | PoS tagging |\n",
    "| ðŸ’¾ | <b>Checkpoint</b> | checkpoint_filename | Checkpoint (tagged frames) to file |\n",
    "| ðŸ”¨ | TaggedFrame<b>ToTokens</b> | extract_tagged_tokens_opts, filter_opts | Tokens extractor | User\n",
    "| ðŸ”¨ | <b>TokensTransform</b> | tokens_transform_opts | Tokens transformer | User\n",
    "| ðŸ”¨ | <b>Vocabulary</b> | âšª | Generate a token to integer ID mapping | âšª\n",
    "| ðŸ”¨ | <b>ToDocumentContentTuple</b> | âšª | API adapter| âšª\n",
    "| ðŸ”¨ | <i>Partition</i> | âšª | Partition corpus into subsets based on predicate | 'year'\n",
    "| ðŸ”Ž | <i>ToTTM</i> | âšª | Transform each partition to TTM matrices | User\n",
    "| ðŸ”¨ | <b>ToCoOccurrence</b> | âšª| Transform TTM into data frame with normalized values | âšª\n",
    "| ðŸ’¾ | <i>Checkpoint</i> | checkpoint_filename| Store co-occurrence data frame | âšª\n",
    "| ðŸ”¨ | <b>ToDTM</b> | vectorize_opts| Transform data frame into DTM | âšª\n",
    "| ðŸ’¾ | <b>Checkpoint</b> | checkpoint_filename| Checkpoint (DTM) to file | âšª\n",
    "\n",
    "\n",
    "### How the co-occurrence counts are computed\n",
    "\n",
    "The co-occurrences are computed using a sliding window of size (D + 1 + D) that word by word moves through each document in the corpus, and keeps count of how many windows each pair of words co-occur in. Note that all windows will (currenty) always have an odd number of words, and the reason for this is the conditioned co-occurrence described below.\n",
    "\n",
    "The computation is done by first creating a (streamed) windowed corpus, consisting of all windows. From this corpus a DTM (document-term-matrix) is created, giving counts for each word in each window. This DTM is then used to compute a TTM (term-term-matrix) simply by multiplying the DTM with a transposed version of itself.\n",
    "\n",
    "Also note that the process of generating windows currently ignores sentences, paragraphs etc.\n",
    "\n",
    "#### GUI elements\n",
    "\n",
    "| | Config element |  Description |\n",
    "| -- | :------------- | :------------- |\n",
    "| | Corpus type | Type of corpus, disabled since only text corpora are allowed in this notebook.\n",
    "| | Source corpus file | Select file (ZIP) or folder that contains the text documents.\n",
    "| | Output tag | String that will be prefix to result files. Only valid filename chars are allowed.\n",
    "| | Output folder | Target folder for result files.\n",
    "| | Part-of-speech groups | Groups of tags to include in DTM given corpus PoS-schema\n",
    "| | Filename fields | Specifies attribute values to be extracted from filenames\n",
    "| | Lemmatize | Use word's lemma (base form)\n",
    "| | To lower | Lowercase all words\n",
    "| | Only alphabetic | Filter out words that has at least one non-alphabetic character\n",
    "| | Only alphanumeric | Filter out words that has at least one non-alphanumeric character\n",
    "| | No stopwords | Remove common stopwords using NLTK language specific stopwords\n",
    "| | Extra stopwords | Additional stopwords to remove\n",
    "| | Create subfolder | Sore result in subfolder named `output tag` in `target folder`\n",
    "| | Context distance | Max allowed distance to window's center-most word\n",
    "| | Concept | If specified, filter out windows having a center-most word not in list of specified words\n",
    "| | Remove concept | If specified, filter out concept word's co-occurrence pairs\n",
    "| | Normalize | Normalize output based on selected time period's corpus size\n",
    "| | Smooth | Smooth curve using PCHIP interpolations.\n",
    "\n",
    "\n",
    "N.B. Note that PoS schema (e.g. SUC, Universal, ON5 Penn Treebank tag sets) and language must be set for each corpus. This, and other options, is specified in the _corpus configuration file_. For an example, please see _SSI.yaml_ inf the `resources` folder.\n",
    "\n",
    "\n",
    "### Concept co-occurrence\n",
    "\n",
    "The algorithm allows for computing a conditioned co-occurrence, where the set of windows are constrained so that the center-most word must one of a number of specified (concept) words. This results in a set of co-occurrences that occur in close proximity (i.e. the max distance of D) of the center-most word.\n",
    "\n",
    "### Word trends\n",
    "\n",
    "Specifiy words of interest in the textbox in the text box. You can use both __wildcards__ ```*``` and __regular expressions__ to widen your search. The\n",
    "words in the vocabulary that matches what you have specified will be listed in the selection box. Since using wildcards and regexps can result\n",
    "in a large number of words, only the `Word count` most frequent words are displayed. Refine your search if you get to many matches.\n",
    "\n",
    "The words are plotted when selected in the selection box. You can select and plot __multiple words__ by pressing CTRL, or using CTRL + arrow keys. The words are listed  in descending order by global corpus frequency.\n",
    "\n",
    "The regular expressions must be surrounded by vertical bars ```|```. To find words ending with ```tion```\n",
    "you can enter ```|.*tion$|```  The vertical\n",
    "bars is specified only so that the system can distinguish the regexp from normal words. The actual expression is ```^.*tion$```.\n",
    "The dot and star ```.*``` matches any character (the dot) any number of times (the ```*```). The dollar sign ```$``` indicates the word ending.\n",
    "So this expression matches all words that begins with any number of characters follwoed, by the character sequence ```tion``` at the end of the word.\n",
    "To match all words starting with `info`you can enter ```|^info.*|``` where ```^``` specifies the start of the word.\n",
    "\n",
    "The \"words\" in this case are co-occurrence pairs and to find instances matching \"information\" you could enter ```information*```, ```*information``` or ```*information*``` to match pairs starting with information, ending with information or containing information respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bokeh.plotting import output_notebook\n",
    "from IPython.core.display import display\n",
    "from penelope.notebook.co_occurrence import main_gui\n",
    "\n",
    "import __paths__\n",
    "\n",
    "output_notebook()\n",
    "gui = main_gui.MainGUI(\n",
    "    corpus_config=\"riksdagens-protokoll\",\n",
    "    corpus_folder=__paths__.corpus_folder,\n",
    "    data_folder=__paths__.data_folder,\n",
    "    resources_folder=__paths__.resources_folder,\n",
    "    global_count_threshold=250,\n",
    ")\n",
    "display(gui.layout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a205d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
