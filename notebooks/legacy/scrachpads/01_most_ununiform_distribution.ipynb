{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "psychological-synthetic",
   "metadata": {},
   "source": [
    "# Find word's that changes most over time\n",
    "\n",
    "## Approach\n",
    "\n",
    "Compare word's distribution over time with a uniform distribution. Use as null hypothesis the belief that a word's distribution does not change over time. Filter out all the words for which there is no significance.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Goodness_of_fit#Categorical_data\n",
    "\n",
    "\n",
    "## Candidate methods\n",
    "\n",
    "See: https://stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution\n",
    "\n",
    "\n",
    "### DONE Chi-square test for a discrete uniform distribution\n",
    "\n",
    "A χ2 goodness-of-fit test is used to determine how (un)likely a data serie (i.e. the word's distribution over time) has been generate by a (discrete) uniform distribution. The actual word counts for each year are used since χ2 is not applicable to relative frequencies. As a rule of thumb, χ2 test requires each individual value to be greater or equal to 5.\n",
    "\n",
    "From [stats.stackexchange.com](stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution):\n",
    "\n",
    "*If you have not only the frequencies but the actual counts, you can use a χ2 goodness-of-fit test for each data series. In particular, you wish to use the test for a discrete uniform distribution. This gives you a good test, which allows you to find out which data series are likely not to have been generated by a uniform distribution, but does not provide a measure of uniformity..... (I guess that the chi-squared statistic can be seen as a measure of uniformity, but it has some drawbacks, such as the lack of convergence, dependence on the arbitrarily placed bins, that the number of expected counts in the cells needs to be sufficiently large, etc. Which measure/test to use is a matter of taste though, and entropy is not without its problems either (in particular, there are many different estimators of the entropy of a distribution). To me, entropy seems like a less arbitrary measure and is easier to interpret.)*\n",
    "\n",
    " $\\tilde{\\chi}^2=\\frac{1}{d}\\sum_{k=1}^{n} \\frac{(O_k - E_k)^2}{E_k}$ (d degree of freedom, n samples, E expected, O observed)\n",
    "\n",
    "References:\n",
    "\n",
    "  - [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "  - [comparing-two-word-distributions](stats.stackexchange.com/questions/236192/comparing-two-word-distributions)\n",
    "\n",
    "DONE ### Simple linear regression\n",
    "\n",
    "Use least-squares fit to compute a Compare word's distribution over time with a uniform distribution. Use as null hypothesis the belief that a word's distribution does not change over time. Filter out all the words for which there is no significance.\n",
    "\n",
    "| Slope | $y = k * x + m$ | Use linear regression to compute slope k. Select n word having highest absoulute value |\n",
    "\n",
    "### G-test for a discrete uniform distribution\n",
    "\n",
    "en.wikipedia.org/wiki/G-test\n",
    "\n",
    "### Kolmogorov-Smirnov test (KS-test)\n",
    "\n",
    "stackoverflow.com/questions/25208421/how-to-test-for-uniformity\n",
    "\n",
    "### Entropy\n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_%28information_theory%29\n",
    "\n",
    "\n",
    "*There are other possible approaches, such as computing the entropy of each series - the uniform distribution maximizes the entropy, so if the entropy is suspiciously low you would conclude that you probably don't have a uniform distribution. That works as a measure of uniformity in some sense.*\n",
    "\n",
    "### Kullback-Leibler divergence (KS-test)\n",
    "Another suggestion would be to use a measure like the Kullback-Leibler divergence, which measures the similarity of two distributions.\n",
    "\n",
    "\n",
    "### L2 norm\n",
    "\n",
    "*Here is a simple heuristic: if you assume elements in any vector sum to 1 (or simply normalize each element with the sum to achieve this), then uniformity can be represented by L2 norm, which ranges from 1d√ to 1, with d being the dimension of vectors. The lower bound 1d√ corresponds to uniformity and upper bound to the 1-hot vector. To scale this to a score between 0 and 1, you can use n∗d√−1d√−1, where n is the L2 norm.\n",
    "\n",
    "```\n",
    "def gof_by_l2_norm(matrix, axis=1):\n",
    "    d = matrix.shape[int(not axis)]\n",
    "    l2_norm = (np.linalg.norm(matrix, axis=axis) * math.sqrt(d) - 1 ) / (math.sqrt(d) - 1)\n",
    "    return l2_norm\n",
    "```\n",
    "\n",
    "https://stats.stackexchange.com/questions/248772/why-does-the-l2-norm-heuristic-work-in-measuring-uniformity-of-probability-distr?noredirect=1&lq=1\n",
    "\n",
    "### Earth Mover Distance\n",
    "\n",
    "*The earth mover distance, also known as the Wasserstein metric, measures the distance between two histograms. Essentially, it considers one histogram as a number of piles of dirt and then assesses how much dirt one needs to move and how far (!) to turn this histogram into the other. You would measure the distance between your distribution and a uniform one over the days of the week.* (https://stats.stackexchange.com/a/178187)\n",
    "\n",
    "\n",
    "* [variance - How does one measure the non-uniformity of a distribution? - Cross Validated](https://stats.stackexchange.com/questions/25827/how-does-one-measure-the-non-uniformity-of-a-distribution)\n",
    "* [G-test - Wikipedia](https://en.wikipedia.org/wiki/G-test#Relation_to_the_chi-squared_test)\n",
    "* [g-test uniform distribution python - Sök på Google](https://www.google.com/search?q=g-test+uniform+distribution+python&oq=g-test+&aqs=chrome.0.69i59l2j69i57j0j69i60l2.3775j0j1&sourceid=chrome&ie=UTF-8)\n",
    "* [Python String Format Cookbook – mkaz.blog](https://mkaz.blog/code/python-string-format-cookbook/)\n",
    "* [The writing and reporting of assertions in tests](https://docs.pytest.org/en/2.8.7/assert.html)\n",
    "\n",
    "* [scipy.stats.entropy — SciPy v1.3.1 Reference Guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)\n",
    "* [python - Interpreting scipy.stats.entropy values - Stack Overflow](https://stackoverflow.com/questions/26743201/interpreting-scipy-stats-entropy-values)\n",
    "* [Plotting data with matplotlib — How to Think Like a Computer Scientist: Learning with Python 3](https://howtothink.readthedocs.io/en/latest/PvL_H.html)\n",
    "* [lint-analysis/33-cluster-auto.ipynb at master · davidmcclure/lint-analysis](https://github.com/davidmcclure/lint-analysis/blob/master/notebooks/2017/33-cluster-auto.ipynb)\n",
    "* [Search · sm.OLS fit.predict plot](https://github.com/search?p=2&q=sm.OLS+fit.predict+plot&type=Code)\n",
    "* [statsmodels.regression.linear_model.OLS — statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS)\n",
    "* [Scikit-learn Normalization mode (L1 vs L2 & Max) - Cross Validated](https://stats.stackexchange.com/questions/225564/scikit-learn-normalization-mode-l1-vs-l2-max)\n",
    "* [Pyplot tutorial — Matplotlib 3.1.1 documentation](https://matplotlib.org/3.1.1/tutorials/introductory/pyplot.html)\n",
    "* [matplotlib axis range - Sök på Google](https://www.google.com/search?q=matplotlib+axis+range&oq=matplotlib+axis&aqs=chrome.3.69i57j0l5.6542j0j4&sourceid=chrome&ie=UTF-8)\n",
    "* [patsy - Describing statistical models in Python — patsy 0.5.1+dev documentation](https://patsy.readthedocs.io/en/latest/)\n",
    "* [k-nearest neighbors algorithm - Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n",
    "* [chi squared - Comparing two word distributions - Cross Validated](https://stats.stackexchange.com/questions/236192/comparing-two-word-distributions)\n",
    "* [Chi-squared test - Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "* [scipy - Calculating probability distribution from time series data in python - Stack Overflow](https://stackoverflow.com/questions/49293019/calculating-probability-distribution-from-time-series-data-in-python)\n",
    "* [scipy.stats.gaussian_kde — SciPy v1.3.1 Reference Guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html)\n",
    "* [What is the difference between trendline and regression line? - Quora](https://www.quora.com/What-is-the-difference-between-trendline-and-regression-line)\n",
    "* [How is the k-nearest neighbor algorithm different from k-means clustering? | Python Tutorial](https://pythonprogramminglanguage.com/How-is-the-k-nearest-neighbor-algorithm-different-from-k-means-clustering/)\n",
    "* [[1510.00012] Fast Discrete Distribution Clustering Using Wasserstein Barycenter with Sparse Support](https://arxiv.org/abs/1510.00012)\n",
    "* [Exceed Synonyms, Exceed Antonyms | Merriam-Webster Thesaurus](https://www.merriam-webster.com/thesaurus/exceed)\n",
    "* [python - Swapping 1 with 0 and 0 with 1 in a Pythonic way - Stack Overflow](https://stackoverflow.com/questions/1779286/swapping-1-with-0-and-0-with-1-in-a-pythonic-way/1779448#1779448)\n",
    "\n",
    "* [Issues · humlab-sead/sead_query_api](https://github.com/humlab-sead/sead_query_api/issues)\n",
    "* [scipy.stats.chi2_contingency — SciPy v0.14.0 Reference Guide](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.chi2_contingency.html)\n",
    "* [Capturing of the stdout/stderr output — pytest documentation](https://docs.pytest.org/en/latest/capture.html)\n",
    "* [sm.OLS np matrix - Sök på Google](https://www.google.com/search?q=sm.OLS+np+matrix&oq=sm.OLS+np+matrix&aqs=chrome..69i57.11997j0j4&sourceid=chrome&ie=UTF-8)\n",
    "* [scipy.stats.chi2_contingency — SciPy v1.3.1 Reference Guide](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html#scipy.stats.chi2_contingency)\n",
    "* [G-test - Wikipedia](https://en.wikipedia.org/wiki/G-test)\n",
    "* [OsloMovers/Web.csproj at bd8a6c91aa957bf8a4857eaccbc4b383c711ddfb · webmasterdevlin/OsloMovers](https://github.com/webmasterdevlin/OsloMovers/blob/bd8a6c91aa957bf8a4857eaccbc4b383c711ddfb/Web/Web.csproj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=wrong-import-position\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import bokeh\n",
    "import pandas as pd\n",
    "import penelope.corpus.dtm as vectorized_corpus\n",
    "import penelope.utility as utility\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from penelope.common import goodness_of_fit as gof\n",
    "\n",
    "sys.path = ['/home/roger/source/welfare_state_analytics'] + sys.path\n",
    "\n",
    "\n",
    "logger = utility.get_logger()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-plane",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "v_y_corpus = (\n",
    "    vectorized_corpus.VectorizedCorpus.load(\n",
    "        tag='SOU_1945-1989_L0_+N_+S', folder='/home/roger/source/welfare_state_analytics/output'\n",
    "    )\n",
    "    .group_by_year()\n",
    "    .slice_by_n_count(10000)\n",
    "    .slice_by_n_top(100000)\n",
    ")\n",
    "\n",
    "v_ny_corpus = v_y_corpus.normalize(axis=1).normalize(axis=0)\n",
    "\n",
    "v_m_corpus = v_y_corpus.normalize(axis=1, keep_magnitude=True)\n",
    "\n",
    "_ = v_ny_corpus.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-peter",
   "metadata": {},
   "source": [
    "Compute (ordinary least square) linear regression on corpus nomalized by each years volume, but keeping the magnitude.callable\n",
    "The first year's values are kept, while rest of the years are scaled based on each year's token count compared to first years token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-values",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "df_fits = gof.compute_goddness_of_fits_to_uniform(v_y_corpus)\n",
    "df_fits.sort_values('l2_norm', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-connectivity",
   "metadata": {},
   "source": [
    "Two axes:\n",
    "https://stackoverflow.com/questions/25199665/one-chart-with-two-different-y-axis-ranges-in-bokeh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-billion",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_word_distribution(\n",
    "    x_corpus: vectorized_corpus.VectorizedCorpus, df: pd.DataFrame\n",
    "):  # pylint: disable=too-many-locals\n",
    "\n",
    "    tokens: List[str] = list(df.token)\n",
    "\n",
    "    colors = itertools.cycle(bokeh.palettes.Dark2[8])\n",
    "\n",
    "    min_year: int = x_corpus.document_index.year.min()\n",
    "    max_year: int = x_corpus.document_index.year.max()\n",
    "\n",
    "    years = [str(y) for y in range(min_year, max_year + 1)]\n",
    "\n",
    "    tokens_data = {token: x_corpus.get_word_vector(token) for token in tokens}\n",
    "\n",
    "    tokens_data['year'] = years\n",
    "    tokens_data['current'] = tokens_data[tokens[0]]\n",
    "\n",
    "    source = bokeh.models.ColumnDataSource(tokens_data)\n",
    "\n",
    "    # for token in tokens:\n",
    "    #    color = next(colors)\n",
    "    #    p.circle(x='year', y=token, size=5,source=source, color=color)\n",
    "    #    p.line(x='year', y=token, source=source, color=color)\n",
    "\n",
    "    color = next(colors)\n",
    "\n",
    "    plot = figure(plot_width=800, plot_height=400, title=\"Word distribution\", x_range=years, name=\"token_plot_01\")\n",
    "\n",
    "    plot.xgrid.grid_line_color = None\n",
    "    plot.xaxis.axis_label = \"Year\"\n",
    "    plot.xaxis.major_label_orientation = 1.2\n",
    "\n",
    "    _ = plot.circle(x='year', y='current', size=5, source=source, color=color)\n",
    "    _ = plot.line(x='year', y='current', source=source, color=color)\n",
    "\n",
    "    # line_source = bokeh.models.ColumnDataSource({'x0': [], 'y0': [], 'x1': [], 'y1': []})\n",
    "    # sr = p.segment(x0='x0', y0='y0', x1='x1', y1='y1', color='olive', alpha=0.6, line_width=3, source=line_source, )\n",
    "\n",
    "    slider = bokeh.models.Slider(start=0, end=len(tokens) - 1, value=0, step=1, title=\"Word\")\n",
    "    next_button = bokeh.models.Button(label=\">>\")\n",
    "\n",
    "    token_vectors = [tokens_data[w] for w in tokens]\n",
    "\n",
    "    update_plot = bokeh.models.CustomJS(\n",
    "        args=dict(source=source, tokens=tokens, token_vectors=token_vectors, title=plot.title),\n",
    "        code=\"\"\"\n",
    "        try {\n",
    "            // var plot = Bokeh.documents[0].get_model_by_name('token_plot_01');\n",
    "            const data = source.data;\n",
    "            const token_index = cb_obj.value;\n",
    "            const y = data['current'];\n",
    "            for (let i = 0; i < y.length; i++)\n",
    "                y[i] = token_vectors[token_index][i];\n",
    "            div.text = tokens[token_index];\n",
    "            title.text = tokens[token_index].toUpperCase();\n",
    "            //div.text = plot.title;\n",
    "            source.change.emit();\n",
    "        } catch (ex) {\n",
    "            div.text = ex.toString();\n",
    "        }\n",
    "    \"\"\",\n",
    "    )\n",
    "\n",
    "    next_plot = bokeh.models.CustomJS(\n",
    "        args=dict(slider=slider),\n",
    "        code=\"\"\"\n",
    "        try {\n",
    "            slider.value = slider.value + 1;\n",
    "            slider.trigger('change');\n",
    "        } catch (ex) {\n",
    "            div.text = ex.toString();\n",
    "        }\n",
    "    \"\"\",\n",
    "    )\n",
    "    # find_token = CustomJS(args=dict(title=plot.title, token2id), code=\"\"\"\n",
    "    #     token = cb_obj.value\n",
    "    # \"\"\")\n",
    "    # text = TextInput(title='Enter title', value='my sine wave', callback=update_title)\n",
    "\n",
    "    div = bokeh.models.Div(text=\"\", width=400, height=10)\n",
    "    # update_plot.args['div'] = div\n",
    "    # update_plot.args['plot'] = plot\n",
    "    slider.js_on_change('value', update_plot)\n",
    "    next_button.js_on_click(next_plot)\n",
    "\n",
    "    w = bokeh.layouts.grid([div, next_button, plot, slider])  # , sizing_mode='stretch_both')\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "widget = plot_word_distribution(v_ny_corpus, df_fits.head(2000))\n",
    "output_file('word_browse.html', title='change title')\n",
    "show(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chi2_stats, chi2_pvalues = list(\n",
    "    zip(*[gof.gof_chisquare_to_uniform(v_m_corpus.data[:, i]) for i in range(0, v_m_corpus.data.shape[1])])\n",
    ")\n",
    "dx = pd.DataFrame({'chisquare': chi2_stats, 'pvalues': chi2_pvalues})\n",
    "dx.chisquare.hist(bins=1000)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
