{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rubber-opinion",
   "metadata": {},
   "source": [
    "## Word Significance using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-lambda",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=wrong-import-position\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Mapping\n",
    "\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import penelope.corpus.dtm as vectorized_corpus\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "root_folder = os.path.abspath(os.path.join(globals()[\"_dh\"][-1], \"../..\"))\n",
    "\n",
    "corpus_folder = os.path.join(root_folder, \"output\")\n",
    "\n",
    "sys.path = [root_folder] + sys.path\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-minority",
   "metadata": {},
   "source": [
    "## Load previously vectorized corpus\n",
    "# Use the `corpus_vectorizer` module to create a new corpus with different settings.\n",
    "\n",
    "The loaded corpus is processed in the following ways:\n",
    "\n",
    " - Exclude tokens having a total word count less than 10\n",
    " - Include at most 50000 most frequent words words.\n",
    "\n",
    "Compute a new TF-IDF weighted corpus\n",
    "\n",
    " - Group document index by year\n",
    " - Compute mean TF-IDF for each year\n",
    "\n",
    "Some references:\n",
    "\n",
    " - [scikit-learn TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)\n",
    " - [Spark MLlib TF-IDF](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-going",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# pylint: disable=wrong-import-position\n",
    "\n",
    "v_corpus: vectorized_corpus.VectorizedCorpus = (\n",
    "    vectorized_corpus.VectorizedCorpus.load(tag=\"SOU_1945-1989_NN+VB+JJ_lemma_L0_+N_+S\", folder=corpus_folder)\n",
    "    .slice_by_n_count(10)\n",
    "    .slice_by_n_top(500000)\n",
    ")\n",
    "\n",
    "tf_idf_corpus = v_corpus.tf_idf().group_by_year(aggregate=\"mean\", fill_gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-playback",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_top_terms(data):\n",
    "\n",
    "    if data is None:\n",
    "        logger.info(\"No data to display!\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame({k: [x[0] for x in v] for k, v in data.items()})\n",
    "\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def compute_top_terms(x_corpus: vectorized_corpus.VectorizedCorpus, n_top: int, idx_groups=None) -> Mapping:\n",
    "\n",
    "    data = {x[\"label\"]: x_corpus.get_top_n_words(n=n_top, indices=x[\"indices\"]) for x in idx_groups}\n",
    "    return data\n",
    "\n",
    "\n",
    "def display_gui(x_corpus: vectorized_corpus.VectorizedCorpus, x_documents_index: pd.DataFrame):\n",
    "\n",
    "    lw = lambda w: ipywidgets.Layout(width=w)\n",
    "\n",
    "    year_min, year_max = x_documents_index.year.min(), x_documents_index.year.max()\n",
    "    years = list(range(year_min, year_max + 1))\n",
    "    decades = [10 * decade for decade in range((year_min // 10), (year_max // 10) + 1)]\n",
    "    lustrums = [lustrum for lustrum in range(year_min - year_min % 5, year_max - year_max % 5, 5)]\n",
    "\n",
    "    groups = [\n",
    "        (\n",
    "            \"year\",\n",
    "            [{\"label\": str(year), \"indices\": [year - year_min]} for year in years],\n",
    "        ),\n",
    "        (\n",
    "            \"decade\",\n",
    "            [\n",
    "                {\n",
    "                    \"label\": str(decade),\n",
    "                    \"indices\": [year - year_min for year in range(decade, decade + 10) if year_min <= year <= year_max],\n",
    "                }\n",
    "                for decade in decades\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"lustrum\",\n",
    "            [\n",
    "                {\n",
    "                    \"label\": str(lustrum),\n",
    "                    \"indices\": [\n",
    "                        year - year_min for year in range(lustrum, lustrum + 5) if year_min <= year <= year_max\n",
    "                    ],\n",
    "                }\n",
    "                for lustrum in lustrums\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    w_n_top = ipywidgets.IntSlider(\n",
    "        description=\"#words\",\n",
    "        min=10,\n",
    "        max=1000,\n",
    "        value=100,\n",
    "        tooltip=\"Number of words to compute\",\n",
    "    )\n",
    "    w_compute = ipywidgets.Button(description=\"Compute\", icon=\"\", button_style=\"Success\", layout=lw(\"120px\"))\n",
    "    w_output = ipywidgets.Output()  # layout={'border': '1px solid black'})\n",
    "    w_groups = ipywidgets.Dropdown(options=groups, value=groups[0][1], description=\"Groups:\")\n",
    "\n",
    "    boxes = ipywidgets.VBox(\n",
    "        [\n",
    "            ipywidgets.HBox(\n",
    "                [w_n_top, w_groups, w_compute],\n",
    "                layout=ipywidgets.Layout(align_items=\"flex-end\"),\n",
    "            ),\n",
    "            w_output,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    display(boxes)\n",
    "\n",
    "    def compute_callback_handler(*_):\n",
    "        w_output.clear_output()\n",
    "        with w_output:\n",
    "            try:\n",
    "\n",
    "                w_compute.disabled = True\n",
    "\n",
    "                data = compute_top_terms(\n",
    "                    x_corpus,\n",
    "                    n_top=w_n_top.value,\n",
    "                    idx_groups=w_groups.value,\n",
    "                )\n",
    "\n",
    "                display_top_terms(data)\n",
    "\n",
    "            except Exception as ex:\n",
    "                logger.error(ex)\n",
    "            finally:\n",
    "                w_compute.disabled = False\n",
    "\n",
    "    w_compute.on_click(compute_callback_handler)\n",
    "\n",
    "\n",
    "display_gui(tf_idf_corpus, tf_idf_corpus.document_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-glasgow",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_word(x_corpus: vectorized_corpus.VectorizedCorpus, word: str):\n",
    "    wv = x_corpus.get_word_vector(word)\n",
    "\n",
    "    df = pd.DataFrame({\"count\": wv, \"year\": x_corpus.document_index.year}).set_index(\"year\")\n",
    "    df.plot()\n",
    "\n",
    "\n",
    "plot_word(v_corpus, \"arbete\")\n",
    "plot_word(tf_idf_corpus, \"arbete\")\n",
    "# plot_word(yearly_tf_idf_corpus, \"arbete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-anthropology",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# np.nansum(a, axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n",
    "# np.nanmean(a, axis=None, dtype=None, out=None, keepdims=<no value>)[source]\n",
    "docs = [\n",
    "    \"the house had a tiny little mouse\",\n",
    "    \"the cat saw the mouse\",\n",
    "    \"the mouse ran away from the house\",\n",
    "    \"the cat finally ate the mouse\",\n",
    "    \"the end of the mouse story\",\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "tfidf = tfidf_vectorizer_vectors.todense()\n",
    "# TFIDF of words not in the doc will be 0, so replace them with nan\n",
    "tfidf[tfidf == 0] = np.nan\n",
    "# Use nanmean of numpy which will ignore nan while calculating the mean\n",
    "means = np.nansum(tfidf, axis=0)\n",
    "# convert it into a dictionary for later lookup\n",
    "means = dict(zip(tfidf_vectorizer.get_feature_names(), means.tolist()[0]))\n",
    "\n",
    "tfidf = tfidf_vectorizer_vectors.todense()\n",
    "# Argsort the full TFIDF dense vector\n",
    "ordered = np.argsort(tfidf * -1)\n",
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "top_k = 5\n",
    "for i, doc in enumerate(docs):\n",
    "    result = {}\n",
    "    # Pick top_k from each argsorted matrix for each doc\n",
    "    for t in range(top_k):\n",
    "        # Pick the top k word, find its average tfidf from the\n",
    "        # precomputed dictionary using nanmean and save it to later use\n",
    "        result[words[ordered[i, t]]] = means[words[ordered[i, t]]]\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
